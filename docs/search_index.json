[
["index.html", "Speedy Scholar 1 Preface", " Speedy Scholar Deblina Mukherjee 2020-09-27 1 Preface This is a place for me to put notes on academic books/papers I read. Idea originally from Ayman Nadeem’s project by the same name, but implemented using Yuhui Xie’s bookdown package, and Sean Kross’s boilerplate. "],
["critical-race-theory-for-hci-ihudiya-finda-ogbonnaya-ogburu-angela-d-r-smith-alexandra-to-kentaro-toyama-.html", "Critical Race Theory for HCI, Ihudiya Finda Ogbonnaya-Ogburu, Angela D. R. Smith, Alexandra To, Kentaro Toyama.", " Critical Race Theory for HCI, Ihudiya Finda Ogbonnaya-Ogburu, Angela D. R. Smith, Alexandra To, Kentaro Toyama. Critical race theory as a way to advance inclusive research and also reduce HCI community’s racial disparities Theoretical concepts from CRT: ordinariness of racism, social construction of race, interest convergence (i.e. people in power support progressive goals only when it serves their selfish interst), intersectionality, critique of liberalism, uniqueness of voices of color, and storytelling as a means of exploring oppression Acknowledged limitation: only consider American context of race Also burden of representation on work concerning marginalized group Suggestions: Work with and about communities of color Critiquing and exmining racial bias in technology (as a first step – race should not be an afterthought, strive for reflexsivity and other-concious) HCI is allied research to CRT (HCI aims to design technological artifacts that work for everyone) "],
["the-ethnographer-and-the-algorithm-beyond-the-black-box-angèle-christin-.html", "The Ethnographer and the Algorithm: Beyond the Black Box, Angèle Christin.", " The Ethnographer and the Algorithm: Beyond the Black Box, Angèle Christin. Three strategies for algorithmic ethnography: Algorithmic refraction (reconfigurations that come from socio-technical interactions) Algorithmic comparison (similarity and difference approach to identify unique features) Algorithmic triangulation (using algorithms to get rich qualitative data, address saturation, positionality, and disengagement) Algorithms conventionally thought to be opaque “black boxes” (understood in terms of input and output, without knowledge of internal workings) Opaque because of: intentional secrecy (companies keep secrets / intellectual property) technical illiteracy unintelligible evolution sheer size Others say algorithms not fit in neatly technical boxes They do algorithmic audits, cultural and historical critiques, and ethnography as methodological strategies Ethnography aims to understand the representations, practices and cultures of the people being analyzed “Technologies-in-use” paradigm (studies practices and representations of users) Black boxing can be analyzed as an artefact of scientific and technological legitimacy Latour – “scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need to focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become” Thus, focus on substitutions and associations within assemblages of humans and non-humans Enrollment key in this framework: analyze the dynamics of association, translation, and entanglement that take place whenever humans and non-humans interact Intressment a form of incentivization, part of enrollment From Malcolm, on dialetical relationship between journalist and criminal: “Every journalist who is not too stupid or full of himself to notice what is going on knows that what he does is morally indefensible. He is a kind of confidence man, preying on people’s vanity, ignorance, or loneliness, gaining their trust and betraying them without remorse” does not account for the collective and institutional dynamics shaping the relationship between ethnographers and their informants (?) central value of contemporary ethnographic research is to try to make explicit as much of the research process as possible for the ethnographic community as a whole Humans in the loop, evolving forms of social responsibility within automated systems "],
["the-axiomatic-basis-for-computer-programming-c-a-r-hoare-.html", "The Axiomatic Basis for Computer Programming, C.A.R. Hoare.", " The Axiomatic Basis for Computer Programming, C.A.R. Hoare. Axioms \\(\\rightarrow\\) Theorems "],
["how-not-to-how-not-to-analyze-data-john-w-tukey-against-the-mechanization-of-statistical-inference-alexander-campolo-.html", "How Not to How Not to Analyze Data: John W. Tukey Against the Mechanization of Statistical Inference, Alexander Campolo.", " How Not to How Not to Analyze Data: John W. Tukey Against the Mechanization of Statistical Inference, Alexander Campolo. Password: Evidence2020 Post-WWII / late 1900s: sciences defined knowledge as “the inference from sample to population” Evidence evaluated by null hypothesis testing and significance at the 5% level Tangential: many criticisms of threshold (arbitrary, artificial “straw-person” in null hypothesis, based on incoherent synthesis of different methods, causes estrangement from effect size and statistical power) How did statistical tests become ossified into mechanical, context-independent procedures? Porter shows behavioral sciences need to initiate new members and defend against external critics Tukey did not have this problem – he was a singular elite Tukey evokes values underlying rationale behind use of tests Did not envy physics, did not want the reductionist “mathematization of nature”, did not want extension of Weberian rationality Wanted judgement, experience and even pluralism Tukey advocated for empiricism based in perception and sense Manifesto called Data Analysis and Behavioral Science or Learning to Bear the Quantitative Man’s Burden by Shunning Badmandments Three criticisms of significance tests: sanctify results, reduce to binary decision (signal vs. noise), crowd out other analytical tools “Probabilistic worlds require both techniques for producing knowledge under conditions of uncertainty and, as Theodore Porter has shown, institutions for producing trust, perhaps even faith, in such knowledge.” “…subjective judgement, driven by empirical perceptions, must shepherd quantification across threacherous spans” Tukey was friends with Claude Shannon Tukey gave data an agentive voice “After the single-minded pursuit of optimality, “the next fetish to be attacked,” he suggested with some relish, “is the fetish of objectivity,” understood as “the fallacy that to a single body of data there corresponds a unique appropriate analysis.” Not procedural, mechanical, optimal valuation of objectivity but a communicative raitonality Orit Halpern vs. Daston, Galison, and Porter Tukey had a historical understanding of science Urged behavioral sciences to look to early days of elder sciences - Still problems with Tukey: ideas provide little consensus, connected to him, reflect an elite sensibility (mechanical forms of inference do not have this problem) Produces a “scientific self” from Gallison and Daston "],
["regulatory-entreprenuership-elizabeth-pollman-and-jordan-berry-.html", "Regulatory Entreprenuership, Elizabeth Pollman and Jordan Berry.", " Regulatory Entreprenuership, Elizabeth Pollman and Jordan Berry. Describes a line of business in which changing the law is a significant part of the business plan. Cites (well-funded, scalable, highly connected startups with mass appeal) like AirBnB, Uber, Tesla, the UFC, marijuana dispensaries and DraftKings as agents of regulatory change. Legislative change is achieved with: an initial breaking/operating in a grey area of the law (asking forgiveness not permission) which continues while the company grows “too big to ban”, and leverages their stakeholders to make the issues in question as publically salient as possible. Contrast with regulatory arbitrageurs, who takes law as a given and make minor alterations to their behavior Regulatory entreprenuers prefer executive and legislative bodies to judicial ones, which are less likely to be swayed by public opinion / an army of users. Silicon Valley is “known to foster a certain libertarian-leaning, freemarket ideology that views technology that appeals to the masses as democratic. Given this confluence of factors, it is not surprising that startups might be inclined to start a line of business fraught with legal uncertainty” “Operational deference” also bestowed on tech companies (as a function of public perception) Businesses not constrained by geography, while politics is "],
["simulation-and-similarityusing-models-to-understand-the-world-michael-weisberg-.html", "Simulation and Similarity:Using Models to Understand the World, Michael Weisberg.", " Simulation and Similarity:Using Models to Understand the World, Michael Weisberg. Models are indirect study of real-world systems, different from other forms of representation and analysis, interpreted structures Three types of modelling: mathematical, computational, concrete concrete are physical objects whose physical properties can potentially stand in representational relationships with real-world phenomena mathematical are abstract structures whose properties can potentially stand in relations to mathematical representations of phenomena can’t be assessed or measured directly, so what we learn comes from manipulating the equations that describe it either set-theoretic predicates or sets of trajectories in state space computational models are sets of procedures that can potentially stand in relation to computational description of the behavior of a system Doesn’t take model organisms, verbal models, and idealized exemplars because they fit into above taxonomy Is taxonomy sociological, ontological, or epistemic Models not always veredical (do not always truthfully describe all aspects of their targets) Feyerabend’s dictum: “Anything goes in science” Agent-based model means that each individual is explicitly represented Model descriptors are the words, equations, and pictures that describe a model Giere said model descriptions were the set of statements that define a model Construals provide interpretation for the model’s structure (assignment, scope, fidelity criteria) two kinds of fidelity criteria: dynamic and representational dynamic tells us how close the output of the model must be to the output of real world phenomenon representational gives us standards for evaluating how well the structure of the model maps onto the target system of interest Fictionalist accounts vs. Maths accounts Idealization is the intentional introduction of distortion into scientific representations Modelling as theorhetical practice "]
]
